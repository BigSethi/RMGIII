{"nbformat":4,"nbformat_minor":0,"metadata":{"interpreter":{"hash":"e111ae3915571b6edef858e479b1eab8559f9d45ac36164e1de90516ae344ee5"},"kernelspec":{"name":"python3712jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462","display_name":"Python 3.7.12 64-bit"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"orig_nbformat":4,"metadata":{"interpreter":{"hash":"4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"}},"colab":{"name":"Word2Vec.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"_ExufYbRtKIr","outputId":"574a533e-963a-43fb-88d2-7f844558d231"},"source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n","from tensorflow.keras.layers import TextVectorization\n","import tqdm \n","import os\n","import io\n","\n","print(tf.__version__)\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["2.7.0\n"]}]},{"cell_type":"code","metadata":{"id":"eMcTPYZEtKIw"},"source":["seed = 39\n","vocab_size = 100000\n","num_ns = 4\n","sequence_length = 25\n","embedding_dim = 300\n","batch_size = 1024\n","window_size = 2\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"kjl6QqyetKIw"},"source":["# Generates skip-gram pairs with negative sampling for a list of sequences\n","# (int-encoded sentences) based on window size, number of negative samples\n","# and vocabulary size.\n","sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n","\n","def generate_training_data(sequence):\n","  # Elements of each training example are appended to these lists.\n","  targets, contexts, labels = [], [], []\n","\n","  sequence = sequence.numpy()\n"," \n","  positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n","          sequence,\n","          vocabulary_size=vocab_size,\n","          sampling_table=sampling_table,\n","          window_size=window_size,\n","          negative_samples=0)\n","\n","  for target_word, context_word in positive_skip_grams:\n","    context_class = tf.expand_dims(\n","          tf.constant([context_word], dtype=\"int64\"), 1)\n","    negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n","          true_classes=context_class,\n","          num_true=1,\n","          num_sampled=num_ns,\n","          unique=True,\n","          range_max=vocab_size,\n","          seed=seed,\n","          name=\"negative_sampling\")\n","\n","      # Build context and label vectors (for one target word)\n","    negative_sampling_candidates = tf.expand_dims(\n","        negative_sampling_candidates, 1)\n","\n","    context = tf.concat([context_class, negative_sampling_candidates], 0)\n","    label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n","\n","      # Append each element from the training example to global lists.\n","    targets.append(target_word)\n","    contexts.append(context)\n","    labels.append(label)\n","\n","  return targets, contexts, labels"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"wuXpIDvAtKIx","outputId":"6e1a5f19-d3dd-40d8-d88f-9a217f8413c8"},"source":["\n","conservative_ds = tf.data.TextLineDataset('conservative.txt')\n","liberal_ds = tf.data.TextLineDataset('liberal.txt')\n","\n","\n","text_ds = tf.data.Dataset.sample_from_datasets([conservative_ds, liberal_ds])\n","\n","text_ds = text_ds.map(lambda x: tf.strings.split(x, maxsplit=1)[1]).batch(batch_size)\n","\n","\n","vectorize_layer = TextVectorization(\n","    max_tokens = vocab_size,\n","    output_mode = 'int', \n","    output_sequence_length = sequence_length\n",")\n","\n","vectorize_layer.adapt(text_ds)\n","\n","inverse_vocab = vectorize_layer.get_vocabulary()\n","print(inverse_vocab[:60])\n","\n","\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['', '[UNK]', 'the', 'trump', 'to', 'a', 'of', 'and', 'in', 'that', 'his', 'for', 'on', 'he', 'is', 'with', 'donald', 'said', 'as', 'president', 'has', 'was', 'at', 'by', 'it', 'who', 'from', 'be', 'not', 'but', 'an', 'have', 'about', 'would', 'had', 'will', 'campaign', 'this', 'are', 'i', 'full', 'or', 'him', 'text', 'new', 'after', 'they', 'if', 'one', 'more', 'when', 'house', 'republican', 'clinton', 'its', 'been', 'what', 'which', 'their', 'out']\n"]}]},{"cell_type":"code","metadata":{"id":"CXcmSScstKIy"},"source":["text_vector_ds = text_ds.prefetch(tf.data.AUTOTUNE).map(vectorize_layer).unbatch()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"2SOImRD4tKIy","outputId":"fdfd105f-9109-4c84-f155-dc8feb116cc4"},"source":["\n","def tf_generate_training_data(x):\n","    targets, contexts, labels = tf.py_function(generate_training_data, inp=[x], \n","                                    Tout = [tf.int64, tf.int64, tf.int64])\n","    targets = tf.ensure_shape(targets, [targets.shape[0]])\n","    contexts = tf.ensure_shape(contexts, [targets.shape[0], num_ns+1, 1])\n","    labels = tf.ensure_shape(labels, [targets.shape[0], num_ns+1])\n","\n","    return ((targets, contexts), labels)\n","\n","\n","skip_gram_ds = text_vector_ds.map(tf_generate_training_data, num_parallel_calls=tf.data.AUTOTUNE,       deterministic = False)\n","                                       \n","\n","\n","BATCH_SIZE = 1024\n","BUFFER_SIZE = 10000\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","\n","dataset = skip_gram_ds.unbatch()\n","dataset = dataset.cache().prefetch(buffer_size = AUTOTUNE)\n","\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","print(dataset.element_spec)\n","\n","\n","\n","\n"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5, 1), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))\n"]}]},{"cell_type":"code","metadata":{"id":"6MiR8zTrtKIz"},"source":["class Word2Vec(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super().__init__()\n","\n","        self.target_embedding = Embedding(vocab_size,\n","        embedding_dim,\n","        input_length=1,\n","        name=\"w2v_embedding\"\n","        )\n","\n","        self.context_embedding = Embedding(vocab_size,\n","        embedding_dim,\n","        input_length=num_ns+1\n","        )\n","\n","    def call(self, pair):\n","         target, context = pair\n","\n","         if len(target.shape) == 2:\n","            target = tf.squeeze(target, axis=1)\n","\n","         word_emb = self.target_embedding(target)\n","\n","         context_emb = self.context_embedding(context)\n","\n","         context_emb = tf.squeeze(context_emb, axis = 2)\n","\n","         dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n","\n","         return dots\n","        "],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"2nlXPHOxtKI0","outputId":"d4832c10-4737-4c5f-ed60-b612fde243f1"},"source":["\n","word2Vec = Word2Vec(vocab_size, embedding_dim)\n","word2Vec.compile(optimizer = 'adam', \n","                 loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True),\n","                 metrics = ['accuracy'])\n","\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"word2vec_logs\")\n","\n","word2Vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n"]},{"output_type":"error","ename":"InvalidArgumentError","evalue":"2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Shape of tensor EagerPyFunc:1 [0] is not compatible with expected shape [?,5,1].\n\t [[{{node EnsureShape_1}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_6]]\n  (1) INVALID_ARGUMENT:  Shape of tensor EagerPyFunc:1 [0] is not compatible with expected shape [?,5,1].\n\t [[{{node EnsureShape_1}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_8752]\n\nFunction call stack:\ntrain_function -> train_function\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-d70800a40a96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtensorboard_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"word2vec_logs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mword2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Shape of tensor EagerPyFunc:1 [0] is not compatible with expected shape [?,5,1].\n\t [[{{node EnsureShape_1}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_6]]\n  (1) INVALID_ARGUMENT:  Shape of tensor EagerPyFunc:1 [0] is not compatible with expected shape [?,5,1].\n\t [[{{node EnsureShape_1}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_8752]\n\nFunction call stack:\ntrain_function -> train_function\n"]}]},{"cell_type":"code","metadata":{"id":"upkA_h95tKI0"},"source":["weights = word2Vec.get_layer('w2v_embedding').get_weights()[0]\n","vocab = vectorize_layer.get_vocabulary()\n","\n","out_v = io.open('w2vVectors.tsv', 'w', encoding='utf-8')\n","out_m = io.open('w2Vmetadata.tsv', 'w', encoding='utf-8')\n","\n","for index, word in enumerate(vocab):\n","  if index == 0:\n","    continue  # skip 0, it's padding.\n","  vec = weights[index]\n","  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n","  out_m.write(word + \"\\n\")\n","out_v.close()\n","out_m.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhbs68JHtKI1","outputId":"32c01bca-35f1-4300-ae38-dbb47e1370d2"},"source":["print(len(word2Vec.get_layer('w2v_embedding').get_weights()[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10000\n"]}]},{"cell_type":"code","metadata":{"id":"kNXWC-9ItKI1"},"source":["np.save('w2vVectors.npy', weights)"],"execution_count":null,"outputs":[]}]}
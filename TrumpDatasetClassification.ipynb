{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 230710 files belonging to 2 classes.\n",
      "Using 184568 files for training.\n",
      "Found 230710 files belonging to 2 classes.\n",
      "Using 46142 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 39\n",
    "sequence_length = 25\n",
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "\n",
    "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"dataset\", batch_size = batch_size, validation_split = 0.2, \n",
    "    subset = 'training', seed = seed\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"dataset\", batch_size = batch_size, validation_split = 0.2, \n",
    "    subset = 'validation', seed = seed\n",
    ")\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size =  AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size =  AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens = vocab_size,\n",
    "    output_mode = 'int', \n",
    "    output_sequence_length = sequence_length)\n",
    "\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/25\n",
      "181/181 [==============================] - 38s 141ms/step - loss: 0.6897 - accuracy: 0.5009 - val_loss: 0.6867 - val_accuracy: 0.4925\n",
      "Epoch 2/25\n",
      "181/181 [==============================] - 23s 126ms/step - loss: 0.6830 - accuracy: 0.5043 - val_loss: 0.6820 - val_accuracy: 0.5045\n",
      "Epoch 3/25\n",
      "181/181 [==============================] - 20s 112ms/step - loss: 0.6774 - accuracy: 0.5155 - val_loss: 0.6789 - val_accuracy: 0.5176\n",
      "Epoch 4/25\n",
      "181/181 [==============================] - 22s 124ms/step - loss: 0.6719 - accuracy: 0.5267 - val_loss: 0.6763 - val_accuracy: 0.5246\n",
      "Epoch 5/25\n",
      "181/181 [==============================] - 21s 115ms/step - loss: 0.6662 - accuracy: 0.5384 - val_loss: 0.6740 - val_accuracy: 0.5319\n",
      "Epoch 6/25\n",
      "181/181 [==============================] - 21s 114ms/step - loss: 0.6603 - accuracy: 0.5493 - val_loss: 0.6712 - val_accuracy: 0.5371\n",
      "Epoch 7/25\n",
      "181/181 [==============================] - 23s 125ms/step - loss: 0.6544 - accuracy: 0.5604 - val_loss: 0.6695 - val_accuracy: 0.5424\n",
      "Epoch 8/25\n",
      "181/181 [==============================] - 21s 115ms/step - loss: 0.6484 - accuracy: 0.5705 - val_loss: 0.6683 - val_accuracy: 0.5457\n",
      "Epoch 9/25\n",
      "181/181 [==============================] - 21s 116ms/step - loss: 0.6425 - accuracy: 0.5797 - val_loss: 0.6676 - val_accuracy: 0.5508\n",
      "Epoch 10/25\n",
      "181/181 [==============================] - 21s 117ms/step - loss: 0.6365 - accuracy: 0.5885 - val_loss: 0.6667 - val_accuracy: 0.5536\n",
      "Epoch 11/25\n",
      "181/181 [==============================] - 23s 125ms/step - loss: 0.6304 - accuracy: 0.5969 - val_loss: 0.6662 - val_accuracy: 0.5629\n",
      "Epoch 12/25\n",
      "181/181 [==============================] - 21s 116ms/step - loss: 0.6243 - accuracy: 0.6051 - val_loss: 0.6668 - val_accuracy: 0.5704\n",
      "Epoch 13/25\n",
      "181/181 [==============================] - 21s 117ms/step - loss: 0.6181 - accuracy: 0.6128 - val_loss: 0.6669 - val_accuracy: 0.5773\n",
      "Epoch 14/25\n",
      "181/181 [==============================] - 23s 125ms/step - loss: 0.6120 - accuracy: 0.6197 - val_loss: 0.6665 - val_accuracy: 0.5819\n",
      "Epoch 15/25\n",
      "181/181 [==============================] - 22s 123ms/step - loss: 0.6057 - accuracy: 0.6267 - val_loss: 0.6676 - val_accuracy: 0.5870\n",
      "Epoch 16/25\n",
      "181/181 [==============================] - 23s 125ms/step - loss: 0.5993 - accuracy: 0.6342 - val_loss: 0.6684 - val_accuracy: 0.5890\n",
      "Epoch 17/25\n",
      "181/181 [==============================] - 22s 124ms/step - loss: 0.5927 - accuracy: 0.6415 - val_loss: 0.6702 - val_accuracy: 0.5869\n",
      "Epoch 18/25\n",
      "181/181 [==============================] - 21s 117ms/step - loss: 0.5861 - accuracy: 0.6484 - val_loss: 0.6735 - val_accuracy: 0.5871\n",
      "Epoch 19/25\n",
      "181/181 [==============================] - 22s 124ms/step - loss: 0.5796 - accuracy: 0.6553 - val_loss: 0.6760 - val_accuracy: 0.5871\n",
      "Epoch 20/25\n",
      "181/181 [==============================] - 23s 125ms/step - loss: 0.5729 - accuracy: 0.6619 - val_loss: 0.6770 - val_accuracy: 0.5882\n",
      "Epoch 21/25\n",
      "181/181 [==============================] - 21s 118ms/step - loss: 0.5662 - accuracy: 0.6687 - val_loss: 0.6781 - val_accuracy: 0.5899\n",
      "Epoch 22/25\n",
      "181/181 [==============================] - 22s 119ms/step - loss: 0.5593 - accuracy: 0.6749 - val_loss: 0.6858 - val_accuracy: 0.5870\n",
      "Epoch 23/25\n",
      "181/181 [==============================] - 23s 125ms/step - loss: 0.5523 - accuracy: 0.6815 - val_loss: 0.6901 - val_accuracy: 0.5871\n",
      "Epoch 24/25\n",
      "181/181 [==============================] - 21s 117ms/step - loss: 0.5451 - accuracy: 0.6880 - val_loss: 0.6925 - val_accuracy: 0.5896\n",
      "Epoch 25/25\n",
      "181/181 [==============================] - 23s 125ms/step - loss: 0.5379 - accuracy: 0.6942 - val_loss: 0.6994 - val_accuracy: 0.5899\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2c26fa2410>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class EmbeddingInitializer(tf.keras.initializers.Initializer):\n",
    "    def __call__(self, shape=None, dtype=None, **kwargs):\n",
    "        weights = tf.convert_to_tensor(np.load('w2vVectors.npy'))\n",
    "        return weights\n",
    "\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer = EmbeddingInitializer(),\n",
    "                            mask_zero= True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    vectorize_layer,\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"classifier_logs\")\n",
    "\n",
    "model.fit(train_ds, epochs=25,\n",
    "                    validation_data=val_ds,\n",
    "                    validation_steps=30, \n",
    "                    callbacks = tensorboard_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory ‘checkpoints’: File exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!cd checkpoints/\n",
    "!mkdir one \n",
    "model.save_weights(\"./checkpoints/one/mycheckpoint\")\n",
    "with open(\"checkpoints/one/METADATA.txt\", 'w') as file:\n",
    "    file.write(\"Batch size = \" + str(batch_size))\n",
    "    file.write(\"Seed = \" + str(seed))\n",
    "    file.write(\"Sequence length = \" + str(sequence_length))\n",
    "    file.write(\"Vocab size = \" + str(vocab_size))\n",
    "    file.write(\"Embedding dimensions = \" + str(embedding_dim))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The sentence 'rasekhi left for iran to visit her family the day after trump was elected president her roommate agatha lyczek said ashdkfahsd asdfkasdhfkja asdhfakhsdfj dakjhfaksjdf adhfjasd sahdfkjasd ' is conservative\n"
     ]
    }
   ],
   "source": [
    "sentence = 'rasekhi left for iran to visit her family the day after trump was elected president her roommate agatha lyczek said ashdkfahsd asdfkasdhfkja asdhfakhsdfj dakjhfaksjdf adhfjasd sahdfkjasd '\n",
    "guess = model.predict([sentence])[0][0]\n",
    "if guess > 0.5:\n",
    "    label = 'liberal'\n",
    "else:\n",
    "    label = 'conservative'\n",
    "\n",
    "print('The sentence ' + \"'\" + sentence + \"'\" + ' is ' + label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e111ae3915571b6edef858e479b1eab8559f9d45ac36164e1de90516ae344ee5"
  },
  "kernelspec": {
   "name": "python3712jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462",
   "display_name": "Python 3.7.12 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "metadata": {
   "interpreter": {
    "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
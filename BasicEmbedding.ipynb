{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import os\n",
    "import io\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_conservative = tf.keras.utils.get_file(\"conservative.txt\", \n",
    "\"https://github.com/JerryWei03/NewB/raw/master/conservative.txt\",\n",
    "                                  cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "dataset_liberal = tf.keras.utils.get_file(\"liberal.txt\", \n",
    "\"https://github.com/JerryWei03/NewB/raw/master/liberal.txt\",\n",
    "                                  cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/JerryWei03/NewB/raw/master/test.txt\n",
      "1556480/1548599 [==============================] - 0s 0us/step\n",
      "1564672/1548599 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "dataset_test = tf.keras.utils.get_file(\"test.txt\", \n",
    "\"https://github.com/JerryWei03/NewB/raw/master/test.txt\",\n",
    "                                  cache_dir='.',\n",
    "                                  cache_subdir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"conservative.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    counter = 0\n",
    "    lines = f.readlines()\n",
    "    for s in lines:\n",
    "        name = \"dataset/conservative/\" + str(counter) + \".txt\"\n",
    "        with open(name, 'w', encoding='utf-8') as d:\n",
    "            d.write(s.split(maxsplit=1)[1])\n",
    "        counter += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"liberal.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    counter = 0\n",
    "    lines = f.readlines()\n",
    "    for s in lines:\n",
    "        name = \"dataset/liberal/\" + str(counter) + \".txt\"\n",
    "        with open(name, 'w', encoding='utf-8') as d:\n",
    "            d.write(s.split(maxsplit=1)[1])\n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average line length: 24.01202808720905\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_words = 0\n",
    "conservative_lines = 0\n",
    "liberal_lines = 0\n",
    "with open(\"conservative.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    conservative_lines = len(lines)\n",
    "    for s in lines:\n",
    "        total_words += len(s.split())\n",
    "\n",
    "with open(\"liberal.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    liberal_lines = len(lines)\n",
    "    for s in lines:\n",
    "        total_words += len(s.split())\n",
    "    \n",
    "\n",
    "    print(\"Average line length: \" + str(total_words / (conservative_lines + liberal_lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create both datasets with conservative labeled 0 and liberal labeled 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 230710 files belonging to 2 classes.\n",
      "Using 184568 files for training.\n",
      "Found 230710 files belonging to 2 classes.\n",
      "Using 46142 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 39\n",
    "\n",
    "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"dataset\", batch_size = batch_size, validation_split = 0.2, \n",
    "    subset = 'training', seed = seed\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"dataset\", batch_size = batch_size, validation_split = 0.2, \n",
    "    subset = 'validation', seed = seed\n",
    ")\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size =  AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size =  AUTOTUNE)\n",
    "\n",
    "sequence_length = 24\n",
    "vocab_size = 10000\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens = vocab_size,\n",
    "    output_mode = 'int', \n",
    "    output_sequence_length = sequence_length)\n",
    "\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "181/181 [==============================] - 146s 802ms/step - loss: 0.6887 - accuracy: 0.5009 - val_loss: 0.6796 - val_accuracy: 0.4972\n",
      "Epoch 2/15\n",
      "181/181 [==============================] - 5s 27ms/step - loss: 0.6642 - accuracy: 0.5368 - val_loss: 0.6633 - val_accuracy: 0.5492\n",
      "Epoch 3/15\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 0.6430 - accuracy: 0.5817 - val_loss: 0.6604 - val_accuracy: 0.5646\n",
      "Epoch 4/15\n",
      "181/181 [==============================] - 4s 24ms/step - loss: 0.6325 - accuracy: 0.5992 - val_loss: 0.6618 - val_accuracy: 0.5707\n",
      "Epoch 5/15\n",
      "181/181 [==============================] - 6s 30ms/step - loss: 0.6272 - accuracy: 0.6063 - val_loss: 0.6633 - val_accuracy: 0.5722\n",
      "Epoch 6/15\n",
      "181/181 [==============================] - 5s 27ms/step - loss: 0.6243 - accuracy: 0.6100 - val_loss: 0.6646 - val_accuracy: 0.5723\n",
      "Epoch 7/15\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 0.6224 - accuracy: 0.6121 - val_loss: 0.6655 - val_accuracy: 0.5721\n",
      "Epoch 8/15\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 0.6211 - accuracy: 0.6134 - val_loss: 0.6662 - val_accuracy: 0.5721\n",
      "Epoch 9/15\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 0.6202 - accuracy: 0.6142 - val_loss: 0.6668 - val_accuracy: 0.5714\n",
      "Epoch 10/15\n",
      "181/181 [==============================] - 6s 31ms/step - loss: 0.6195 - accuracy: 0.6154 - val_loss: 0.6672 - val_accuracy: 0.5715\n",
      "Epoch 11/15\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 0.6190 - accuracy: 0.6158 - val_loss: 0.6676 - val_accuracy: 0.5717\n",
      "Epoch 12/15\n",
      "181/181 [==============================] - 4s 22ms/step - loss: 0.6185 - accuracy: 0.6162 - val_loss: 0.6678 - val_accuracy: 0.5721\n",
      "Epoch 13/15\n",
      "181/181 [==============================] - 4s 21ms/step - loss: 0.6181 - accuracy: 0.6167 - val_loss: 0.6681 - val_accuracy: 0.5718\n",
      "Epoch 14/15\n",
      "181/181 [==============================] - 4s 23ms/step - loss: 0.6178 - accuracy: 0.6170 - val_loss: 0.6682 - val_accuracy: 0.5719\n",
      "Epoch 15/15\n",
      "181/181 [==============================] - 4s 20ms/step - loss: 0.6175 - accuracy: 0.6170 - val_loss: 0.6684 - val_accuracy: 0.5714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x161ef9e64f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model = Sequential([\n",
    "    vectorize_layer, \n",
    "    Embedding(vocab_size, embedding_dim, name = 'embedding'),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e111ae3915571b6edef858e479b1eab8559f9d45ac36164e1de90516ae344ee5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

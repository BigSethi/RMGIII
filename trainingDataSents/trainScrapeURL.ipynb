{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence Processor\n",
    "\n",
    "def process(url, bias, arg1, arg2, arg3):\n",
    "  \n",
    "  raw = ''\n",
    "  soup = BeautifulSoup(requests.get(url).text, 'lxml')\n",
    "  pgrafs = soup.find_all(arg1, arg2==arg3)\n",
    "  for pgraf in pgrafs:\n",
    "    raw += pgraf.text + \" \"\n",
    "  \n",
    "  goodlets = '''1234567890abcdefghijklmnopqrstuvwxyz\"'.,:;!?/- '''\n",
    "  proc1 = raw.lower()\n",
    "  proc2 = ''\n",
    "  for char in proc1:\n",
    "    if char in goodlets:\n",
    "      proc2 += char\n",
    "  \n",
    "  wordList = proc2.split()\n",
    "  endPunc = '''.?!'''\n",
    "  sentBank = ''\n",
    "  sentLen = 0\n",
    "  sents = []\n",
    "\n",
    "  for word in wordList:\n",
    "      sentBank += word + ' '\n",
    "      sentLen += 1\n",
    "      for char in endPunc:\n",
    "        if char in word:\n",
    "          if sentLen < 30 and sentLen > 5:\n",
    "            sents.append(bias + ' ' + sentBank)\n",
    "          sentBank = ''\n",
    "          sentLen = 0\n",
    "  if len(sents) > 4:\n",
    "    del sents[0]\n",
    "    del sents[0]\n",
    "    del sents[len(sents) - 1]\n",
    "    del sents[len(sents) - 1]\n",
    "\n",
    "  with open('tempAllSents.txt', 'a') as f:\n",
    "    for sent in sents:\n",
    "      f.write(sent + \"\\n\")\n",
    "    \n",
    "  \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vox - SUCCESS\n",
    "\n",
    "#Getting list of archive pages\n",
    "voxArch = []\n",
    "voxArch.append('https://www.vox.com/archives/policy-and-politics/2022/1')\n",
    "voxArch.append('https://www.vox.com/archives/policy-and-politics/2022/2')\n",
    "for x in range(2018, 2022):\n",
    "    for y in range(1, 13):\n",
    "        voxArch.append('https://www.vox.com/archives/policy-and-politics/{}/{}'.format(x, y))\n",
    "\n",
    "#Getting list of urls\n",
    "with open('voxUrls.txt', 'w') as f:\n",
    "    for arch in voxArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        boxes = soup.find_all('h2', class_='c-entry-box--compact__title')\n",
    "        for box in boxes:\n",
    "            f.write(box.find('a')['href'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSNBC\n",
    "\n",
    "#Getting list of archive pages\n",
    "msnArch = []\n",
    "msnArch.append('https://www.msnbc.com/archive/articles/2022/february')\n",
    "msnArch.append('https://www.msnbc.com/archive/articles/2022/january')\n",
    "months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
    "for x in range (2017, 2022):\n",
    "    for y in range(0, 12):\n",
    "        msnArch.append('https://www.msnbc.com/archive/articles/{}/{}'.format(x, months[y]))\n",
    "\n",
    "#Getting list of urls\n",
    "with open('msnUrls.txt', 'w') as f:\n",
    "    for arch in msnArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        lines = soup.find_all('a', class_=False)\n",
    "        for line in lines:\n",
    "            url = line['href']\n",
    "            if ('/opinion' in url and url != 'https://www.msnbc.com/opinion/columnists') or 'maddow-show/' in url:\n",
    "                f.write(url + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Yorker\n",
    "\n",
    "#Getting list of archive pages\n",
    "yorArch = []\n",
    "soup = BeautifulSoup(requests.get('https://www.newyorker.com/sitemap').text, 'lxml')\n",
    "lines = soup.find_all('a', class_=False)\n",
    "for line in lines:\n",
    "    url = line['href']\n",
    "    if 'sitemap?year' in url and len(yorArch) < 200:\n",
    "        yorArch.append(url)\n",
    "\n",
    "#Getting list of urls\n",
    "with open('yorUrls.txt', 'w') as f:\n",
    "    for arch in yorArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        lines = soup.find_all('a', class_=False)\n",
    "        for line in lines:\n",
    "            url = line['href']\n",
    "            if '/news/' in url:\n",
    "                f.write(url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New York Times - SUCCESS\n",
    "\n",
    "#Getting list of archive pages\n",
    "nytArch = []\n",
    "for x in range(2020, 2023):\n",
    "    for y in range(1, 13):\n",
    "        for z in range(1, 10):\n",
    "            if z < 10 and y < 10:\n",
    "                nytArch.append('https://www.nytimes.com/sitemap/{}/0{}/0{}/'.format(x, y, z))\n",
    "            elif z >= 10 and y < 10:\n",
    "                nytArch.append('https://www.nytimes.com/sitemap/{}/0{}/{}/'.format(x, y, z))\n",
    "            else:\n",
    "                nytArch.append('https://www.nytimes.com/sitemap/{}/{}/{}/'.format(x, y, z))\n",
    "\n",
    "#Getting list of urls\n",
    "with open('nytUrls.txt', 'w') as f:\n",
    "    for arch in nytArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        lines = soup.find_all('a', class_=False)\n",
    "        for line in lines:\n",
    "            url = line['href']\n",
    "            if ('/politics/' in url) and '/video' not in url and '/live/' not in url:\n",
    "                f.write(url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CBS - SUCCESS\n",
    "\n",
    "#Getting list of archive pages\n",
    "cbsArch = []\n",
    "for x in range(1, 100):\n",
    "    cbsArch.append('https://www.cbsnews.com/latest/politics/{}/'.format(x))\n",
    "\n",
    "#Getting list of urls\n",
    "with open('cbsUrls.txt', 'w') as f:\n",
    "    for arch in cbsArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        lines = soup.find_all('a', class_='item__anchor')\n",
    "        for line in lines:\n",
    "            url = line['href']\n",
    "            f.write(url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testUrls.txt', 'r') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        process(line, '-1', 'p', 'class_', True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b38b666f41083fba72c74a9e32241ecd05bd2b7ea4aae7cba4a54d3c544805ae"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('3.9.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence Processor\n",
    "\n",
    "def process(urlList, bias, arg1):\n",
    "    with open(urlList, 'r') as f:\n",
    "        while True:\n",
    "            url = f.readline().strip()\n",
    "            if not url:\n",
    "                break\n",
    "\n",
    "            #Get paragraph\n",
    "            raw = ''\n",
    "            soup = BeautifulSoup(requests.get(url).text, 'lxml')\n",
    "            pgrafs = soup.find_all(arg1)\n",
    "            for pgraf in pgrafs:\n",
    "                raw += pgraf.text + ' '\n",
    "            if len(raw) < 20:\n",
    "                return\n",
    "\n",
    "            #Process part 1\n",
    "            goodlets = '''1234567890abcdefghijklmnopqrstuvwxyz\"'.,:;!?/- '''\n",
    "            raw = raw.lower()\n",
    "            proc1 = ''\n",
    "            for char in raw:\n",
    "                if char in goodlets:\n",
    "                    proc1 += char\n",
    "            proc1 = proc1.replace('u.s.', 'us')\n",
    "            proc1 = proc1.replace('dr.', 'dr')\n",
    "            proc1 = proc1.replace('mr.', 'mr')\n",
    "            proc1 = proc1.replace('mrs.', 'mrs')\n",
    "            proc1 = proc1.replace('ms.', 'ms')\n",
    "\n",
    "            #Split into sentences\n",
    "            endPunc = '''.?!'''\n",
    "            sentBank = ''\n",
    "            sentLen = 1\n",
    "            sents = []\n",
    "            for char in proc1:\n",
    "                sentBank += char\n",
    "                if char == ' ':\n",
    "                    sentLen += 1\n",
    "                if char in endPunc:\n",
    "                    if sentLen > 6:\n",
    "                        sents.append(bias + ' ' + sentBank)\n",
    "                    sentBank = ''\n",
    "                    sentLen = 0\n",
    "            \n",
    "            #Process part 2\n",
    "            if len(sents) > 4:\n",
    "                del sents[0]\n",
    "                del sents[0]\n",
    "                del sents[len(sents) - 1]\n",
    "                del sents[len(sents) - 1]\n",
    "\n",
    "            #Write\n",
    "            with open('allSents.txt', 'a') as h:\n",
    "                for sent in sents:\n",
    "                    h.write(sent + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VOX News\n",
    "\n",
    "#Biden\n",
    "voxArch = []\n",
    "for x in range(1, 8):\n",
    "    voxArch.append('https://www.vox.com/biden-administration/archives/{}'.format(x))\n",
    "with open('voxUrls.txt', 'a') as f:\n",
    "    for arch in voxArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        boxes = soup.find_all('h2', class_='c-entry-box--compact__title')\n",
    "        for box in boxes:\n",
    "            url = box.find('a')['href']\n",
    "            if 'biden' in url:\n",
    "                f.write(url + \"\\n\")\n",
    "\n",
    "#Pandemic\n",
    "voxArch = []\n",
    "for x in range(1, 8):\n",
    "    voxArch.append('https://www.vox.com/coronavirus-covid19/archives/{}'.format(x))\n",
    "with open('voxUrls.txt', 'a') as f:\n",
    "    for arch in voxArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        boxes = soup.find_all('h2', class_='c-entry-box--compact__title')\n",
    "        for box in boxes:\n",
    "            url = box.find('a')['href']\n",
    "            if 'covid' in url:\n",
    "                f.write(url + \"\\n\")\n",
    "\n",
    "#Trump\n",
    "voxArch = []\n",
    "for x in range(1, 8):\n",
    "    voxArch.append('https://www.vox.com/donald-trump/archives/{}'.format(x))\n",
    "with open('voxUrls.txt', 'a') as f:\n",
    "    for arch in voxArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        boxes = soup.find_all('h2', class_='c-entry-box--compact__title')\n",
    "        for box in boxes:\n",
    "            url = box.find('a')['href']\n",
    "            if 'trump' in url:\n",
    "                f.write(url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSNBC\n",
    "\n",
    "#Archs\n",
    "msnArch = []\n",
    "mos = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
    "msnArch.append('https://www.msnbc.com/archive/articles/2022/january')\n",
    "msnArch.append('https://www.msnbc.com/archive/articles/2022/february')\n",
    "for x in range(2020, 2022):\n",
    "    for y in range(0, 12):\n",
    "        msnArch.append('https://www.msnbc.com/archive/articles/{}/{}'.format(x, mos[y]))\n",
    "\n",
    "#Urls\n",
    "with open('msnUrls.txt', 'a') as f:\n",
    "    for arch in msnArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        boxes = soup.find_all('a')\n",
    "        for box in boxes:\n",
    "            url = box['href']\n",
    "            if 'trump' in url or 'biden' in url or 'covid' in url:\n",
    "                f.write(url + \"\\n\")\n",
    "\n",
    "counter = 0\n",
    "with open('msnUrls.txt', 'w') as f:\n",
    "    with open('msnUrls1.txt', 'r') as h:\n",
    "        while True:\n",
    "            url = h.readline()\n",
    "            counter += 1\n",
    "            if counter == 3:\n",
    "                f.write(url)\n",
    "                counter = 0\n",
    "            if not url:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New York Times\n",
    "\n",
    "#Archives\n",
    "nytArch = []\n",
    "for x in range(1, 3):\n",
    "    for y in range(1, 24, 3):\n",
    "        nytArch.append('https://www.nytimes.com/sitemap/2022/{}/{}/'.format(x, y))\n",
    "for x in range(2020, 2022):\n",
    "    for y in range(1, 13):\n",
    "        for z in range(1, 28, 3):\n",
    "            nytArch.append('https://www.nytimes.com/sitemap/{}/{}/{}/'.format(x, y, z))\n",
    "\n",
    "#Urls\n",
    "with open('nytUrls.txt', 'a') as f:\n",
    "    for arch in nytArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        boxes = soup.find_all('a')\n",
    "        for box in boxes:\n",
    "            url = box['href']\n",
    "            if '/es/' not in url and '/video/' not in url and '/live/' not in url and ('trump' in url or 'biden' in url or 'covid' in url or 'pandemic' in url):\n",
    "                f.write(url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CBS\n",
    "\n",
    "#Archs\n",
    "cbsArch = []\n",
    "for x in range(1, 801):\n",
    "    cbsArch.append('https://www.cbsnews.com/latest/politics/{}/#'.format(x))\n",
    "\n",
    "#Urls\n",
    "with open('cbsUrls.txt', 'a') as f:\n",
    "    for arch in cbsArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        place = soup.find('div', class_='col-8 nocontent')\n",
    "        boxes = place.find_all('a')\n",
    "        for box in boxes:\n",
    "            url = box['href']\n",
    "            if 'trump' in url or 'biden' in url or 'covid' in url:\n",
    "                f.write(url + \"\\n\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NPR\n",
    "\n",
    "#Archs\n",
    "nprArch = []\n",
    "for x in range(1, 3):\n",
    "    for y in range(1, 24, 3):\n",
    "        nprArch.append('https://www.npr.org/sections/news/archive?date={}-{}-2022'.format(x, y))\n",
    "for x in range(2020, 2022):\n",
    "    for y in range(1, 13):\n",
    "        for z in range(1, 28, 3):\n",
    "            nprArch.append('https://www.npr.org/sections/news/archive?date={}-{}-{}'.format(y, z, x))\n",
    "\n",
    "#Urls\n",
    "with open('nprUrls.txt', 'a') as f:\n",
    "    for arch in nprArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        boxes = soup.find_all('h2', class_='title')\n",
    "        for box in boxes:\n",
    "            url = box.find('a')['href']\n",
    "            if 'trump' in url or 'biden' in url or 'covid' in url or 'pandemic' in url:\n",
    "                f.write(url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newsmax\n",
    "\n",
    "#Archs\n",
    "nmxArch = []\n",
    "for x in range(1, 3):\n",
    "    nmxArch.append('https://www.newsmax.com/archives/politics/1/2022/{}/'.format(x))\n",
    "for x in range(2020, 2022):\n",
    "    for y in range(1, 13):\n",
    "        nmxArch.append('https://www.newsmax.com/archives/politics/1/{}/{}/'.format(x, y))\n",
    "\n",
    "#Urls\n",
    "with open('nmxUrls.txt', 'w') as f:\n",
    "    for arch in nmxArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        boxes = soup.find_all('a', class_=True)\n",
    "        for box in boxes:\n",
    "            url = box['href']\n",
    "            if 'trump' in url or 'biden' in url or 'covid' in url:\n",
    "                f.write('https://newsmax.com' + url + \"\\n\")\n",
    "\n",
    "counter = 0\n",
    "with open('nmxUrls.txt', 'w') as f:\n",
    "    with open('nmxUrls1.txt', 'r') as h:\n",
    "        while True:\n",
    "            url = h.readline()\n",
    "            counter += 1\n",
    "            if counter == 3:\n",
    "                f.write(url)\n",
    "                counter = 0\n",
    "            if not url:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breitbart\n",
    "\n",
    "#Archs\n",
    "bbtArch = []\n",
    "for x in range(1, 255):\n",
    "    bbtArch.append('https://www.breitbart.com/tag/donald-trump/page/{}/'.format(x))\n",
    "    bbtArch.append('https://www.breitbart.com/tag/coronavirus/page/{}/'.format(x))\n",
    "    bbtArch.append('https://www.breitbart.com/tag/joe-biden/page/{}/'.format(x))\n",
    "\n",
    "#Urls\n",
    "with open('bbtUrls.txt', 'w') as f:\n",
    "    for arch in bbtArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        boxes = soup.find_all('div', class_='tC')\n",
    "        for box in boxes:\n",
    "            url = box.find('a')['href']\n",
    "            if '/politics/' in url:\n",
    "                f.write(\"https://breitbart.com\" + url + \"\\n\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#National Review\n",
    "\n",
    "natArch = []\n",
    "for x in range(1, 700):\n",
    "    natArch.append('https://www.nationalreview.com/section/news/page/{}/'.format(x))\n",
    "\n",
    "#Urls\n",
    "with open('natUrls.txt', 'w') as f:\n",
    "    for arch in natArch:\n",
    "        soup = BeautifulSoup(requests.get(arch).text, 'lxml')\n",
    "        boxes = soup.find_all('a')\n",
    "        for box in boxes:\n",
    "            url = box['href']\n",
    "            if 'trump' in url or 'biden' in url or 'covid' in url:\n",
    "                f.write(url + \"\\n\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "process('testUrls.txt', '2', 'p')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b38b666f41083fba72c74a9e32241ecd05bd2b7ea4aae7cba4a54d3c544805ae"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('3.9.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
